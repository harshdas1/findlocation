def calculate_metrics(retrieved_set, relevant_set):
    true_positive = len(retrieved_set.intersection(relevant_set))
    false_positive = len(retrieved_set.difference(relevant_set))
    false_negative = len(relevant_set.difference(retrieved_set))

    print("True Positive:", true_positive)
    print("False Positive:", false_positive)
    print("False Negative:", false_negative)

    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
    f_measure = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0

    return precision, recall, f_measure

# Example data
retrieved_set = {"doc1", "doc2", "doc3"}       # Retrieved (predicted)
relevant_set = {"doc1", "doc4"}                # Relevant (actual)

# Calculate metrics
precision, recall, f_measure = calculate_metrics(retrieved_set, relevant_set)

# Output results
print(f"\nPrecision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F-measure: {f_measure:.2f}")
